name: News Dataset Pipeline

on:
  schedule:
    - cron: '*/15 * * * *'  # Runs every 15 minutes
  workflow_dispatch:        # Allows you to trigger manually for testing

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    permissions:
      contents: write       # CRITICAL: Allows the action to push changes back to your repo
      
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 feedparser huggingface-hub python-dateutil
      
      - name: Run news scraper
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python scripts/scraper.py
      
      - name: Save Data to GitHub
        run: |
          # 1. Configure your identity
          git config --global user.name 'Sachin23991'
          git config --global user.email 'sachinraosahab7@gmail.com'
          
          # 2. Stage the data folder (where the JSONL files live)
          git add data/
          
          # 3. Check if there are changes before committing
          if [[ -n $(git status -s) ]]; then
            git commit -m "Auto-Update: Appended news data [$(date +%Y-%m-%d_%H:%M)]"
            git push
          else
            echo "No new data found. Skipping commit."
          fi
